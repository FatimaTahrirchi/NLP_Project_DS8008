import tqdmimport torch.nn as nnimport csvimport itertoolsimport numpy as npfrom tqdm import tqdmfrom collections import Counterfrom sklearn import utils, metricsimport torch.nn.functional as Fimport torchclass BasicConvResBlock(nn.Module):    def __init__(self, input_dim=128, n_filters=256, kernel_size=3, padding=1, stride=1, shortcut=False, downsample=None):        super(BasicConvResBlock, self).__init__()        self.downsample = downsample        self.shortcut = shortcut        self.conv1 = nn.Conv1d(input_dim, n_filters, kernel_size=kernel_size, padding=padding, stride=stride)        self.bn1 = nn.BatchNorm1d(n_filters)        self.relu = nn.ReLU()        self.conv2 = nn.Conv1d(n_filters, n_filters, kernel_size=kernel_size, padding=padding, stride=stride)        self.bn2 = nn.BatchNorm1d(n_filters)    def forward(self, x):        residual = x        out = self.conv1(x)        out = self.bn1(out)        out = self.relu(out)        out = self.conv2(out)        out = self.bn2(out)        if self.shortcut:            if self.downsample is not None:                residual = self.downsample(x)            out += residual        out = self.relu(out)        return outclass VDCNN(nn.Module):    def __init__(self, n_classes=2, num_embedding=141, embedding_dim=16, depth=9, n_fc_neurons=2048, shortcut=False):        super(VDCNN, self).__init__()        layers = []        fc_layers = []        self.embed = nn.Embedding(num_embedding,embedding_dim, padding_idx=0, max_norm=None, norm_type=2, scale_grad_by_freq=False, sparse=False)        layers.append(nn.Conv1d(embedding_dim, 64, kernel_size=3, padding=1))        if depth == 9:            n_conv_block_64, n_conv_block_128, n_conv_block_256, n_conv_block_512 = 1, 1, 1, 1        elif depth == 17:            n_conv_block_64, n_conv_block_128, n_conv_block_256, n_conv_block_512 = 2, 2, 2, 2        elif depth == 29:            n_conv_block_64, n_conv_block_128, n_conv_block_256, n_conv_block_512 = 5, 5, 2, 2        elif depth == 49:            n_conv_block_64, n_conv_block_128, n_conv_block_256, n_conv_block_512 = 8, 8, 5, 3        layers.append(BasicConvResBlock(input_dim=64, n_filters=64, kernel_size=3, padding=1, shortcut=shortcut))        for _ in range(n_conv_block_64-1):            layers.append(BasicConvResBlock(input_dim=64, n_filters=64, kernel_size=3, padding=1, shortcut=shortcut))          layers.append(nn.MaxPool1d(kernel_size=3, stride=2, padding=1)) # l = initial length / 2        ds = nn.Sequential(nn.Conv1d(64, 128, kernel_size=1, stride=1, bias=False), nn.BatchNorm1d(128))        layers.append(BasicConvResBlock(input_dim=64, n_filters=128, kernel_size=3, padding=1, shortcut=shortcut, downsample=ds))        for _ in range(n_conv_block_128-1):            layers.append(BasicConvResBlock(input_dim=128, n_filters=128, kernel_size=3, padding=1, shortcut=shortcut))        layers.append(nn.MaxPool1d(kernel_size=3, stride=2, padding=1)) # l = initial length / 4        ds = nn.Sequential(nn.Conv1d(128, 256, kernel_size=1, stride=1, bias=False), nn.BatchNorm1d(256))        layers.append(BasicConvResBlock(input_dim=128, n_filters=256, kernel_size=3, padding=1, shortcut=shortcut, downsample=ds))        for _ in range(n_conv_block_256 - 1):            layers.append(BasicConvResBlock(input_dim=256, n_filters=256, kernel_size=3, padding=1, shortcut=shortcut))        layers.append(nn.MaxPool1d(kernel_size=3, stride=2, padding=1))        ds = nn.Sequential(nn.Conv1d(256, 512, kernel_size=1, stride=1, bias=False), nn.BatchNorm1d(512))        layers.append(BasicConvResBlock(input_dim=256, n_filters=512, kernel_size=3, padding=1, shortcut=shortcut, downsample=ds))        for _ in range(n_conv_block_512 - 1):            layers.append(BasicConvResBlock(input_dim=512, n_filters=512, kernel_size=3, padding=1, shortcut=shortcut))        layers.append(nn.AdaptiveMaxPool1d(8))        fc_layers.extend([nn.Linear(8*512, n_fc_neurons), nn.ReLU()])        # layers.append(nn.MaxPool1d(kernel_size=8, stride=2, padding=0))        # fc_layers.extend([nn.Linear(61*512, n_fc_neurons), nn.ReLU()])        fc_layers.extend([nn.Linear(n_fc_neurons, n_fc_neurons), nn.ReLU()])        fc_layers.extend([nn.Linear(n_fc_neurons, n_classes)])        self.layers = nn.Sequential(*layers)        self.fc_layers = nn.Sequential(*fc_layers)        self.__init_weights()    def __init_weights(self):        for m in self.modules():            if isinstance(m, nn.Conv1d):                nn.init.kaiming_normal_(m.weight, mode='fan_in')                if m.bias is not None:                    nn.init.constant_(m.bias, 0)    def forward(self, x):# 128x1024x69 (b X s X f0)        out = self.embed(x)         # 128x1024x16        out = out.transpose(1, 2)   #128x1024x16-->128x16x1024        out = self.layers(out)       #covelutional layers (feature extraction)        out = out.view(out.size(0), -1)   #flatten (After training this output can be used for any ML method)        out = self.fc_layers(out)     #fully connected layers(prediction layer)        return out    ##################################################################################################################################def get_metrics(cm, list_metrics):    """Compute metrics from a confusion matrix (cm)    cm: sklearn confusion matrix    returns:    dict: {metric_name: score}    """    dic_metrics = {}    total = np.sum(cm)    if 'accuracy' in list_metrics:        out = np.sum(np.diag(cm))        dic_metrics['accuracy'] = out/total    if 'pres_0' in list_metrics:        num = cm[0, 0]        den = cm[:, 0].sum()        dic_metrics['pres_0'] =  num/den if den > 0 else 0    if 'pres_1' in list_metrics:        num = cm[1, 1]        den = cm[:, 1].sum()        dic_metrics['pres_1'] = num/den if den > 0 else 0    if 'recall_0' in list_metrics:        num = cm[0, 0]        den = cm[0, :].sum()        dic_metrics['recall_0'] = num/den if den > 0 else 0    if 'recall_1' in list_metrics:        num = cm[1, 1]        den = cm[1, :].sum()        dic_metrics['recall_1'] =  num/den if den > 0 else 0    return dic_metrics####################################################################################################################################def train(epoch,net,dataset,device,msg="val/test",optimize=False,optimizer=None,scheduler=None,criterion=None,list_metrics = ['accuracy']):        net.train() if optimize else net.eval()    epoch_loss = 0    nclasses = len(list(net.parameters())[-1])    cm = np.zeros((nclasses,nclasses), dtype=int)    with tqdm(total=len(dataset),desc="Epoch {} - {}".format(epoch, msg)) as pbar:        for iteration, (tx, ty) in enumerate(dataset):                        data = (tx, ty)            data = [x.to(device) for x in data]            if optimize:                optimizer.zero_grad()            out = net(data[0])            ty_prob = F.softmax(out, 1) # probabilites            #metrics            y_true = data[1].detach().cpu().numpy()            y_pred = ty_prob.max(1)[1].cpu().numpy()            cm += metrics.confusion_matrix(y_true, y_pred, labels=range(nclasses))            dic_metrics = get_metrics(cm, list_metrics)                        loss =  criterion(out, data[1])             epoch_loss += loss.item()            dic_metrics['logloss'] = epoch_loss/(iteration+1)            if optimize:                loss.backward()                optimizer.step()                dic_metrics['lr'] = optimizer.state_dict()['param_groups'][0]['lr']            pbar.update(1)            pbar.set_postfix(dic_metrics)    if scheduler:        scheduler.step()####################################################################################################################def predict(net,dataset,device,msg="prediction"):        net.eval()    y_probs, y_trues = [], []    for iteration, (tx, ty) in tqdm(enumerate(dataset), total=len(dataset), desc="{}".format(msg)):        data = (tx, ty)        data = [x.to(device) for x in data]        out = net(data[0])        ty_prob = F.softmax(out, 1) # probabilites        y_probs.append(ty_prob.detach().cpu().numpy())        y_trues.append(data[1].detach().cpu().numpy())    return np.concatenate(y_probs, 0), np.concatenate(y_trues, 0).reshape(-1, 1)###############################################################################################################def save(net, path):    """    Saves a model's state and it's embedding dic by piggybacking torch's save function    """    dict_m = net.state_dict()    torch.save(dict_m,path)#################################################################################################################